//
// Created by user on 01.02.2023.
//

#include <boost/asio/post.hpp>
#include <windows.h>
#include <cassert>
#include <iostream>
#include <ranges>
#include <filesystem>
#include <chrono>
#include <algorithm>
#include <thread>
#include <atomic>
#include <fstream>
#include <mutex>
#include <sstream>
#include <iomanip>
#include <climits>
#include "InvertedIndex.h"
#include "MyUtils/Encoding.h"
#include "MyUtils/LogFile.h"
#include <boost/algorithm/string.hpp>
#include "simd_tokenizer.h"
#include "OEMFastTokenizer.h"
#include <future>
#include <psapi.h>

static size_t process_memory()
{
    PROCESS_MEMORY_COUNTERS_EX pmc{};
    GetProcessMemoryInfo(GetCurrentProcess(),
                         reinterpret_cast<PROCESS_MEMORY_COUNTERS*>(&pmc),
                         sizeof(pmc));
    return pmc.WorkingSetSize;   // –±–∞–π—Ç—ã
}

static void logIndexError(const std::wstring& path, const std::string& msg)
{
    LogFile::getErrors().write(encoding::wstring_to_utf8(path) + " | " + msg);
}

void inverted_index::InvertedIndex::pingIo(boost::asio::io_context& ctx, const char* name)
{
    inverted_index::InvertedIndex::addToLog(std::string("PING post -> ") + name);
    boost::asio::post(ctx, [name]{
        inverted_index::InvertedIndex::addToLog(std::string("PING executed <- ") + name);
    });
}


void inverted_index::InvertedIndex::commitSingleWord(const PostingTask& t)
{
    const uint32_t wid        = t.wordId;
    const size_t   chunkIndex = wid / CHUNK_SIZE;
    const size_t   localIndex = wid % CHUNK_SIZE;

    // —Å–æ–∑–¥–∞—ë–º/—Ä–∞—Å—à–∏—Ä—è–µ–º —á–∞–Ω–∫ –ø–æ–¥ –≥–ª–æ–±–∞–ª—å–Ω—ã–º –º—å—é—Ç–µ–∫—Å–æ–º, –∫–∞–∫ –≤ getPostingList
    {
        std::lock_guard<std::mutex> g(mapMutex);
        if (chunkIndex >= dictionaryChunks.size())
            dictionaryChunks.resize(chunkIndex + 1);
        if (!dictionaryChunks[chunkIndex])
            dictionaryChunks[chunkIndex] = std::make_unique<Chunk>();
    }

    Chunk& chunk = *dictionaryChunks[chunkIndex];

    std::unique_lock<std::shared_mutex> lk(chunk.mutex);
    chunk.bucket[localIndex][t.fileId] += t.count;
}

void inverted_index::InvertedIndex::commitChunkMap(
        const std::unordered_map<size_t, std::vector<PostingTask>>& chunkMap)
{
    // === 1. (–†–µ–¥–∫–æ) —Ä–∞—Å—à–∏—Ä—è–µ–º –º–∞—Å—Å–∏–≤ —á–∞–Ω–∫–æ–≤ –û–î–ò–ù —Ä–∞–∑ ===
    {
        std::lock_guard<std::mutex> g(mapMutex);

        size_t maxIndex = 0;
        for (auto& [cid, _] : chunkMap)
            if (cid > maxIndex) maxIndex = cid;

        if (maxIndex >= dictionaryChunks.size())
            dictionaryChunks.resize(maxIndex + 1);

        for (auto& [cid, _] : chunkMap)
            if (!dictionaryChunks[cid])
                dictionaryChunks[cid] = std::make_unique<Chunk>();
    }

    // === 2. –û–±–Ω–æ–≤–ª—è–µ–º —á–∞–Ω–∫–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ lock ===
    for (auto& [cid, tasks] : chunkMap)
    {
        Chunk& ch = *dictionaryChunks[cid];
        std::unique_lock<std::shared_mutex> lk(ch.mutex);

        for (auto& t : tasks)
        {
            size_t local = t.wordId % CHUNK_SIZE;
            ch.bucket[local][t.fileId] += t.count;
        }
    }
}


void inverted_index::InvertedIndex::processBatch(const PostingBatch& batch)
{
    try
    {
        if (batch.list.empty())
        {
            if (batch.promise)
                batch.promise->set_value();
            return;
        }

        // 1. LOCAL MAP: wordId ‚Üí count
        robin_hood::unordered_map<uint32_t, uint32_t> widMap;
        widMap.reserve(batch.list.size());

        // 1.1. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–≤–∞ ‚Üí wid + —Å—É–º–º–∏—Ä—É–µ–º counts
        for (auto& [word, count] : batch.list)
        {
            uint32_t wid = wordIds.getId(word);
            widMap[wid] += count;

            // wordRefs –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∑–¥–µ—Å—å (–≤ strand!)
            auto& refv = wordRefs[batch.fileId];
            if (refv.empty() || refv.back() != wid)
                refv.push_back(wid);
        }

        // 2. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ chunkId
        std::unordered_map<size_t, std::vector<PostingTask>> chunkMap;
        chunkMap.reserve(widMap.size());

        for (auto& [wid, count] : widMap)
        {
            size_t chunkIndex = wid / CHUNK_SIZE;
            chunkMap[chunkIndex].push_back({ batch.fileId, wid, count });
        }

        // 3. –û–¥–∏–Ω commit-task –≤ io_commit
        boost::asio::post(cpu_pool_, [this,
                chunkMap = std::move(chunkMap),
                promise = batch.promise]()
        {
            try
            {
                commitChunkMap(chunkMap);
                if (promise) promise->set_value();
            }
            catch (const std::exception& e)
            {
                addToLog(std::string("io_commit handler exception: ") + e.what());
                if (promise) { try { promise->set_exception(std::current_exception()); } catch (...) {} }
            }
            catch (...)
            {
                addToLog("io_commit handler unknown exception");
                if (promise) { try { promise->set_exception(std::current_exception()); } catch (...) {} }
            }
        });

    }
    catch (...)
    {
        if (batch.promise)
            batch.promise->set_exception(std::current_exception());
    }
}


// –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫: –∫–æ–ø–∏—Ä—É–µ—Ç posting-–ª–∏—Å—Ç —Å–ª–æ–≤–∞
std::optional<PostingList> inverted_index::InvertedIndex::getPostingCopyByWord(const std::string& w) const
{
    uint32_t wid;
    if (!wordIds.tryGet(w, wid))
        return std::nullopt;

    const size_t chunkIndex = wid / CHUNK_SIZE;
    const size_t localIndex = wid % CHUNK_SIZE;

    if (chunkIndex >= dictionaryChunks.size())
        return std::nullopt;

    const auto& chunkPtr = dictionaryChunks[chunkIndex];
    if (!chunkPtr)
        return std::nullopt;

    std::shared_lock<std::shared_mutex> lk(chunkPtr->mutex);
    const PostingList& pl = chunkPtr->bucket[localIndex];
    if (pl.empty())
        return std::nullopt;

    return pl; // –∫–æ–ø–∏—è
}


void inverted_index::InvertedIndex::applyBatchInStrand(PostingBatch batch)
{
    // –í–ê–ñ–ù–û: —Ä–µ–∞–ª—å–Ω—ã–π commit –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å—Ç—Ä–æ–≥–æ –≤ strand_,
    // —á—Ç–æ–±—ã –≤—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å dictionary/wordRefs –±—ã–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã.
    boost::asio::post(strand_, [this, batch = std::move(batch)]() mutable
    {
        try {

            processBatch(batch);
        }
        catch (const std::exception& e) {
            addToLog(std::string{"applyBatchInStrand: exception: "} + e.what());
        }
        catch (...) {
            addToLog("applyBatchInStrand: unknown exception");
        }
    });
}

void inverted_index::InvertedIndex::delFromDictionary(const std::vector<FileId>& list)
{
    boost::asio::post(strand_, [this, list]() {
        for (FileId fileId : list)
            safeEraseFile(fileId);   // —Ç–µ–ø–µ—Ä—å –±–µ–∑–æ–ø–∞—Å–Ω–æ, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ commit-–ø–æ—Ç–æ–∫–µ
    });
}


void inverted_index::InvertedIndex::safeEraseFileInternal(FileId fileId)
{
    // –ï—Å–ª–∏ –µ—Å—Ç—å wordRefs ‚Äî –∏–¥—ë–º —Ç–æ–ª—å–∫–æ –ø–æ –Ω—É–∂–Ω—ã–º wid
    auto itRefs = wordRefs.find(fileId);
    if (itRefs != wordRefs.end())
    {
        const auto& widList = itRefs->second;
        for (uint32_t wid : widList)
        {
            size_t chunkIndex = wid / CHUNK_SIZE;
            size_t localIndex = wid % CHUNK_SIZE;

            if (chunkIndex >= dictionaryChunks.size())
                continue;

            auto& chunkPtr = dictionaryChunks[chunkIndex];
            if (!chunkPtr)
                continue;

            Chunk& chunk = *chunkPtr;
            std::unique_lock<std::shared_mutex> lk(chunk.mutex);
            auto& posting = chunk.bucket[localIndex];
            if (!posting.empty())
                posting.erase(fileId);
        }

        wordRefs.erase(itRefs);
    }
    else
    {
        // fallback: –ø–æ–ª–Ω—ã–π –æ–±—Ö–æ–¥ (—Ä–µ–¥–∫–∏–π —Å–ª—É—á–∞–π)
        for (auto& chunkPtr : dictionaryChunks)
        {
            if (!chunkPtr) continue;
            Chunk& chunk = *chunkPtr;

            std::unique_lock<std::shared_mutex> lk(chunk.mutex);
            for (auto& posting : chunk.bucket)
            {
                if (!posting.empty())
                    posting.erase(fileId);
            }
        }
    }

    addToLog("safeEraseFileInternal: removed fileId=" + std::to_string(fileId));
}





std::future<void> inverted_index::InvertedIndex::updateDocumentBase(
        const std::vector<std::wstring>& vecPaths)
{
    // –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤
    std::lock_guard<std::mutex> updateLock(updateMutex);
    
    addToLog("updateDocumentBase() ‚Üí start");
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ª–∏ —É–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
    bool expected = false;
    if (!work.compare_exchange_strong(expected, true)) {
        addToLog("updateDocumentBase() ‚Üí SKIP: update already in progress");
        std::promise<void> p;
        p.set_value();
        return p.get_future();
    }

    // RAII-–æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±—Ä–æ—Å–∞ work –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏
    struct WorkGuard {
        std::atomic<bool>& work_;
        explicit WorkGuard(std::atomic<bool>& w) : work_(w) {}
        ~WorkGuard() { work_.store(false, std::memory_order_release); }
    };
    WorkGuard workGuard(work);

    // 1. diff
    UpdatePack pack = docPaths.getUpdate({ vecPaths.begin(), vecPaths.end() });

    std::vector<FileId> toDelete = pack.removed;
    toDelete.insert(toDelete.end(), pack.updated.begin(), pack.updated.end());

    std::vector<FileId> toIndex = pack.added;
    toIndex.insert(toIndex.end(), pack.updated.begin(), pack.updated.end());

    std::ostringstream diffLog;
    diffLog << "diff: +"  << pack.added.size()
            << ", upd "   << pack.updated.size()
            << ", del "   << pack.removed.size();
    addToLog(diffLog.str());

    // –ù–µ—Ç —Ä–∞–±–æ—Ç—ã ‚Äî —Å—Ä–∞–∑—É –≥–æ—Ç–æ–≤—ã–π future
    if (toDelete.empty() && toIndex.empty())
    {
        std::promise<void> p;
        p.set_value();
        return p.get_future();
        // work –±—É–¥–µ—Ç —Å–±—Ä–æ—à–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ WorkGuard
    }

    // –§–∏–Ω–∞–ª—å–Ω—ã–π promise
    auto finalPromise = std::make_shared<std::promise<void>>();
    std::future<void> future = finalPromise->get_future();

    try
    {
        // ------------------------------------------------------------------
        // 2. –£–î–ê–õ–ï–ù–ò–ï ‚Äî —Å—Ç—Ä–æ–≥–æ –≤ strand_, –Ω–æ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        // ------------------------------------------------------------------
        {
            std::promise<void> delPromise;
            auto delFuture = delPromise.get_future();

            boost::asio::post(strand_, [this, &toDelete, &delPromise]()
            {
                try {
                    for (FileId fileId : toDelete)
                        safeEraseFileInternal(fileId);

                    addToLog("updateDocumentBase: deletions done");
                    delPromise.set_value();
                }
                catch (...) {
                    delPromise.set_exception(std::current_exception());
                }
            });

            delFuture.get(); // ‚Üê –∂–¥—ë–º —É–¥–∞–ª–µ–Ω–∏—è
        }

        // ------------------------------------------------------------------
        // 3. –ò–ù–î–ï–ö–°–ê–¶–ò–Ø ‚Äî CPU pool
        // ------------------------------------------------------------------
        std::vector<FileFuture> fileFutures;
        fileFutures.reserve(toIndex.size());

        // Lock-free —Å—á–µ—Ç—á–∏–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        std::atomic<size_t> processedFiles{0};
        const size_t totalFiles = toIndex.size();
        const size_t progressInterval = std::max<size_t>(100, totalFiles / 20); // –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–µ 5% –∏–ª–∏ 100 —Ñ–∞–π–ª–æ–≤

        for (FileId fileId : toIndex)
        {
            auto p = std::make_shared<std::promise<void>>();

            fileFutures.push_back({
                                          fileId,
                                          docPaths.pathById(fileId),
                                          p->get_future()
                                  });

            boost::asio::post(cpu_pool_,
                              [this, fileId, p, &processedFiles, totalFiles, progressInterval]()
                              {
                                  try {
                                      fileIndexing(fileId, p);
                                      
                                      // Lock-free –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞
                                      size_t processed = processedFiles.fetch_add(1, std::memory_order_relaxed) + 1;
                                      
                                      // –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞)
                                      if (processed % progressInterval == 0 || processed == totalFiles) {
                                          // –ò—Å–ø–æ–ª—å–∑—É–µ–º addToLog —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π
                                          addToLog("Progress: " + std::to_string(processed) + "/" + 
                                                  std::to_string(totalFiles) + " files indexed (" +
                                                  std::to_string((processed * 100) / totalFiles) + "%)");
                                      }
                                  }
                                  catch (...) {
                                      try { p->set_exception(std::current_exception()); } catch (...) {}
                                  }
                              });
        }

        addToLog("updateDocumentBase: indexing tasks dispatched, files=" +
                 std::to_string(fileFutures.size()));

        // ------------------------------------------------------------------
        // 4. –û–ñ–ò–î–ê–ù–ò–ï –í–°–ï–• fileIndexing (–∫–∞–∫ –±—ã–ª–æ, –Ω–æ –±–µ–∑ detached)
        // ------------------------------------------------------------------
        for (auto& ff : fileFutures)
        {
            using namespace std::chrono_literals;

            if (ff.fut.wait_for(60s) != std::future_status::ready)
            {
                logIndexError(ff.path, "TIMEOUT waiting file future (60s)");
                continue;
            }

            try { ff.fut.get(); }
            catch (const std::exception& e) { logIndexError(ff.path, e.what()); }
            catch (...) { logIndexError(ff.path, "unknown exception"); }
        }

        addToLog("updateDocumentBase: all fileIndexing done");

        // ------------------------------------------------------------------
        // 5. –§–ò–ù–ê–õ ‚Äî saveIndex (—Ç—É—Ç –∂–µ, –ª–∏–Ω–µ–π–Ω–æ)
        // ------------------------------------------------------------------
        auto startTime = std::chrono::steady_clock::now();
        addToLog("FINAL: before saveIndex");
        saveIndex();
        auto saveTime = std::chrono::duration_cast<std::chrono::milliseconds>(
            std::chrono::steady_clock::now() - startTime).count();
        addToLog("FINAL: after saveIndex, took " + std::to_string(saveTime) + " ms");

        // Log dictionary statistics and memory usage
        auto stats = getStats();
        size_t memBytes = process_memory();
        size_t dict_size = dictionaryChunks.size() * CHUNK_SIZE;  // Total dictionary slots
        double hole_percent = (dict_size > 0) ? (stats.emptyPostings * 100.0 / dict_size) : 0.0;
        
        std::ostringstream statsLog;
        statsLog << "DICTIONARY STATS: unique_words=" << stats.uniqueWords
                 << ", total_postings=" << stats.totalPostings
                 << ", total_files=" << stats.totalFiles
                 << ", dictionary_slots=" << dict_size
                 << ", holes=" << stats.emptyPostings
                 << ", hole_percent=" << std::fixed << std::setprecision(2) << hole_percent << "%"
                 << ", dictionary_memory=" << (stats.memoryBytes / 1024 / 1024) << " MB"
                 << ", process_memory=" << (memBytes / 1024 / 1024) << " MB";
        addToLog(statsLog.str());

        finalPromise->set_value();
        // work –±—É–¥–µ—Ç —Å–±—Ä–æ—à–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ WorkGuard
    }
    catch (...)
    {
        try { finalPromise->set_exception(std::current_exception()); } catch (...) {}
        // work –±—É–¥–µ—Ç —Å–±—Ä–æ—à–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ WorkGuard
    }

    return future;
}


void logIndexingDebug(const std::string& msg) {
    LogFile::getIndex().write(msg);
}

void inverted_index::InvertedIndex::fileIndexing(
        FileId fileId,
        std::shared_ptr<std::promise<void>> promise)
{
    try
    {
        std::wstring fullPath = docPaths.pathById(fileId); // –ö–û–ü–ò–Ø!

        std::ifstream file(fullPath.c_str(), std::ios::binary);
        if (!file.is_open())
        {
            addToLog("fileIndexing: cannot open file " +
                     encoding::wstring_to_utf8(fullPath));

            promise->set_exception(
                    std::make_exception_ptr(
                            std::runtime_error("file not found")));
            return;
        }

        // --- —Å–ª–æ–≤–∞—Ä—å —á–∞—Å—Ç–æ—Ç ---
        robin_hood::unordered_map<std::string, size_t> freqWordFile;

        // --- SIMD tokenizer ---
        static const size_t BUF_SIZE = 64 * 1024;
        std::vector<char> buf(BUF_SIZE);

        std::string carry;   // —Ö–≤–æ—Å—Ç —Å–ª–æ–≤–∞ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö –±—É—Ñ–µ—Ä–æ–≤

        while (true)
        {
            file.read(buf.data(), BUF_SIZE);
            std::size_t readBytes = file.gcount();
            if (readBytes == 0)
                break;

            const bool isLastChunk = file.eof();

            simd_tokenizer::tokenize_oem866_buffer(
                    buf.data(),
                    readBytes,
                    isLastChunk,
                    carry,
                    [&](std::string_view tok)
                    {
                        // -------- üî• –¢–í–û–Ø OEM-–õ–û–ì–ò–ö–ê --------

                        std::string w;
                        w.assign(tok.data(), tok.size());  // –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º std::string(tok)

                        OEMFastTokenizer::normalizeToken(w);

                        if (!w.empty())
                            ++freqWordFile[w];
                    });
        }

        // —Å–æ–∑–¥–∞—ë–º batch
        PostingBatch batch;
        batch.fileId = fileId;
        batch.promise = promise;

        batch.list.reserve(freqWordFile.size());
        for (auto &[word, count] : freqWordFile)
            batch.list.emplace_back(word, count);

        applyBatchInStrand(std::move(batch));
    }
    catch (...)
    {
        addToLog("fileIndexing: EXCEPTION for fileId=" + std::to_string(fileId));
        try { promise->set_exception(std::current_exception()); }
        catch (...) {}
    }
}


void inverted_index::InvertedIndex::addToLog(const string& _s) {
    LogFile::getIndex().write(_s);
}

PostingList inverted_index::InvertedIndex::getWordCount(const std::string& word)
{
    if (auto opt = getPostingCopyByWord(word))
        return *opt;
    return {};
}

void inverted_index::InvertedIndex::dictonaryToLog() const {
    // for(const auto& i:dictionary)
    //      addToLog(i.first + "\t" + std::to_string(i.second.size()));
}

inverted_index::DictionaryStats inverted_index::InvertedIndex::getStats() const
{
    DictionaryStats stats{};
    
    // –†–µ–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
    stats.uniqueWords = wordIds.size();
    stats.totalFiles = docPaths.size();
    
    // –ü–æ–¥—Å—á–µ—Ç –ø–æ—Å—Ç–∏–Ω–≥–æ–≤ –∏–∑ chunks (–±–µ–∑ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫)
    size_t totalPostings = 0;
    size_t emptyPostings = 0;
    size_t memoryBytes = 0;
    
    // –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ chunks —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞–º–∏
    for (const auto& chunkPtr : dictionaryChunks)
    {
        if (!chunkPtr) {
            // –ü—É—Å—Ç–æ–π chunk = CHUNK_SIZE –ø—É—Å—Ç—ã—Ö –ø–æ—Å—Ç–∏–Ω–≥–æ–≤
            emptyPostings += CHUNK_SIZE;
            continue;
        }
        
        const Chunk& chunk = *chunkPtr;
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º try_lock –¥–ª—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–≥–æ —á—Ç–µ–Ω–∏—è –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ
        std::shared_lock<std::shared_mutex> lk(chunk.mutex);
        
        for (const auto& posting : chunk.bucket)
        {
            size_t postingSize = posting.size();
            if (postingSize == 0) {
                ++emptyPostings;
            } else {
                totalPostings += postingSize;
                // Posting = 6 bytes (uint32_t fileId + uint16_t cnt)
                // + overhead vector (~24 bytes –Ω–∞ –ø—É—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä)
                memoryBytes += postingSize * 6 + 24;
            }
        }
    }
    
    stats.totalPostings = totalPostings;
    stats.emptyPostings = emptyPostings;
    stats.memoryBytes = memoryBytes;
    
    return stats;
}

void inverted_index::InvertedIndex::rebuildDictionaryFromChunks()
{
    std::lock_guard<std::mutex> g(mapMutex);
    dictionary.clear();
    dictionary.reserve(dictionaryChunks.size() * CHUNK_SIZE);

    for (const auto & chunkPtr : dictionaryChunks)
    {
        if (!chunkPtr) {
            // –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ posting-–ª–∏—Å—Ç—ã
            for (size_t i=0; i<CHUNK_SIZE; ++i)
                dictionary.emplace_back();
            continue;
        }

        Chunk& chunk = *chunkPtr;
        std::shared_lock<std::shared_mutex> lk(chunk.mutex);


        for (size_t localIndex = 0; localIndex < CHUNK_SIZE; ++localIndex)
            dictionary.push_back(chunk.bucket[localIndex]);
    }
}

void inverted_index::InvertedIndex::rebuildChunksFromDictionary()
{
    dictionaryChunks.clear();

    for (size_t wid = 0; wid < dictionary.size(); ++wid)
    {
        size_t chunkIndex = wid / CHUNK_SIZE;
        size_t localIndex = wid % CHUNK_SIZE;

        if (chunkIndex >= dictionaryChunks.size())
            dictionaryChunks.resize(chunkIndex + 1);

        if (!dictionaryChunks[chunkIndex])
            dictionaryChunks[chunkIndex] = std::make_unique<Chunk>();

        dictionaryChunks[chunkIndex]->bucket[localIndex] = dictionary[wid];
    }
}




void inverted_index::InvertedIndex::reconstructWordIts()
{
    wordRefs.clear();

    for (size_t chunkIndex = 0; chunkIndex < dictionaryChunks.size(); ++chunkIndex)
    {
        const auto& chunkPtr = dictionaryChunks[chunkIndex];
        if (!chunkPtr) continue;

        Chunk& chunk = *chunkPtr;
        std::shared_lock<std::shared_mutex> lk(chunk.mutex);

        for (size_t localIndex = 0; localIndex < chunk.bucket.size(); ++localIndex)
        {
            const PostingList& posting = chunk.bucket[localIndex];
            if (posting.empty()) continue;

            uint32_t wid = static_cast<uint32_t>(chunkIndex * CHUNK_SIZE + localIndex);

            for (const auto& [docId, cnt] : posting)
                wordRefs[docId].push_back(wid);
        }
    }
}


void inverted_index::InvertedIndex::fixDictionaryHoles()
{

    addToLog("===> Dictionary holes auto-fix started (fixDictionaryHoles)");

    for (FileId fileId = 0; fileId < docPaths.size(); ++fileId)
    {
        const std::wstring& wpath = docPaths.pathById(fileId);


        if (wpath.empty()) continue;

        // –ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –∫–∞–∫ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
        std::ifstream file(wpath, std::ios::binary);
        if (!file.is_open()) continue;

        std::vector<char> data;
        file.unsetf(std::ios::skipws);
        file.seekg(0, std::ios::end);
        size_t fileSize = file.tellg();
        file.seekg(0, std::ios::beg);
        data.reserve(fileSize);

        try {
            data.insert(data.begin(),
                        std::istream_iterator<char>(file),
                        std::istream_iterator<char>());
        } catch (...) { continue; }

        std::stringstream ss;
        std::copy(data.begin(), data.end(), std::ostream_iterator<char>(ss));

        std::set<std::string> realWords;
        auto wordRange = std::ranges::istream_view<std::string>(ss)
                         | std::views::transform([](const std::string& text) {
            auto s = text;
            boost::algorithm::trim_if(s, [](auto c) {
                return OEMCase::iS_not_a_Oem(c) && ispunct(c);
            });
            for (auto& c : s) c = OEMCase::getUpperCharOem(c);
            return s;
        });
        std::ranges::for_each(wordRange, [&realWords](auto&& word) {
            realWords.insert(word);
        });

        std::lock_guard<std::mutex> lock(mapMutex);
        for (const std::string& word : realWords)
        {
            uint32_t wid;
            if (!wordIds.tryGet(word, wid)) continue;

            if (!dictionary[wid].find(fileId)) {
                dictionary[wid][fileId] = 1; // –∏–ª–∏ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π cnt, –µ—Å–ª–∏ –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å
                wordRefs[fileId].push_back(wid);
                addToLog("AUTOFIX: restored word '" + word + "' for file " + std::to_string(fileId));
            }
        }
    }

    addToLog("===> Dictionary holes auto-fix completed (fixDictionaryHoles)");
}


void inverted_index::InvertedIndex::saveIndex() const {
    // –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤ saveIndex
    std::lock_guard<std::mutex> saveLock(saveMutex);
    
    addToLog("saveIndex() ‚Üí start");
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (–Ω–æ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –µ–≥–æ)
    if (work.load(std::memory_order_acquire)) {
        addToLog("saveIndex() ‚Üí SKIP: update in progress");
        return;
    }

    try {
        // –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º dictionary –∏–∑ chunks –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º (–µ—Å–ª–∏ –æ–Ω –ø—É—Å—Ç–æ–π)
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º const_cast –¥–ª—è –≤—ã–∑–æ–≤–∞ –Ω–µ-const –º–µ—Ç–æ–¥–∞
        InvertedIndex* nonConstThis = const_cast<InvertedIndex*>(this);
        {
            std::lock_guard<std::mutex> lock(mapMutex);
            if (dictionary.empty()) {
                nonConstThis->rebuildDictionaryFromChunks();
            }
        }

        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
        std::ofstream ofs("inverted_index3.dat", std::ios::binary);
        if (ofs.is_open()) {
            boost::archive::binary_oarchive oa(ofs);
            {
                std::lock_guard<std::mutex> lock(mapMutex);
                oa << *this;
            }
            ofs.close();
            addToLog("saveIndex() ‚Üí saved successfully");
        } else {
            addToLog("saveIndex() ‚Üí ERROR: Failed to open file for saving index.");
        }

        // –û—á–∏—â–∞–µ–º dictionary –ø–æ—Å–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        {
            std::lock_guard<std::mutex> lock(mapMutex);
            dictionary.clear();
            dictionary.shrink_to_fit();
        }
    }
    catch (const std::exception& e) {
        addToLog("saveIndex() ‚Üí EXCEPTION: " + std::string(e.what()));
    }
    catch (...) {
        addToLog("saveIndex() ‚Üí EXCEPTION: unknown error");
    }
}

inverted_index::InvertedIndex::~InvertedIndex() {
    saveIndex();
}

inverted_index::InvertedIndex::InvertedIndex(boost::asio::thread_pool& cpu_pool, boost::asio::io_context& io_commit)
        : io_commit_(io_commit)
        , strand_(boost::asio::make_strand(io_commit_))
        , cpu_pool_(cpu_pool)
{
        // –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º
        if (std::filesystem::exists("inverted_index3.dat")) {
            // –ï—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            std::ifstream ifs("inverted_index3.dat", std::ios::binary);
            if (ifs.is_open()) {
                boost::archive::binary_iarchive ia(ifs);
                ia >> *this; // –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Boost.Serialization
            }

            dictionary.clear();
            dictionary.shrink_to_fit();

        }
}

void inverted_index::InvertedIndex::compact(double thresholdPercent)
{
    // –í–Ω–µ—à–Ω–∏–π –≤—ã–∑–æ–≤ compact() –¥–æ–ª–∂–µ–Ω –ª–∏—à—å –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–¥–∞—á—É –≤ strand.
    boost::asio::post(strand_, [this, thresholdPercent]()
    {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (–±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å deadlock)
        if (work.load(std::memory_order_acquire)) {
            addToLog("COMPACT: SKIP - update in progress, will retry later");
            return;
        }

        addToLog("COMPACT: started in strand");

        // –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º dictionary –∏–∑ chunks –ø–µ—Ä–µ–¥ compact (–µ—Å–ª–∏ –æ–Ω –ø—É—Å—Ç–æ–π)
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º try_lock –¥–ª—è updateMutex, —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–∞–¥–æ–ª–≥–æ
        std::unique_lock<std::mutex> updateLock(updateMutex, std::try_to_lock);
        if (!updateLock.owns_lock()) {
            addToLog("COMPACT: SKIP - update lock busy, will retry later");
            return;
        }

        // –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        if (work.load(std::memory_order_acquire)) {
            addToLog("COMPACT: SKIP - update started during lock acquisition");
            return;
        }

        if (dictionary.empty()) {
            rebuildDictionaryFromChunks();
        }

        size_t holes = 0;
        for (const auto& post : dictionary)
            if (post.empty())
                ++holes;

        // –ò—Å–ø–æ–ª—å–∑—É–µ–º wordIds.size() –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è
        size_t dict_size = dictionary.size();
        size_t real_words = wordIds.size();
        double hole_percent = (dict_size > 0) ? (holes * 100.0 / dict_size) : 0.0;

        std::ostringstream oss;
        oss << "COMPACT: holes=" << holes
            << ", dict_size=" << dict_size
            << ", real_words=" << real_words
            << ", hole_percent=" << std::fixed << std::setprecision(2)
            << hole_percent << "%"
            << ", threshold=" << std::fixed << std::setprecision(2)
            << thresholdPercent << "%";

        // –ù—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—å compact? –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä–æ–≥ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        if (holes == 0 || hole_percent < thresholdPercent)
        {
            oss << " ‚Üí SKIP (below threshold)";
            addToLog(oss.str());
            return;
        }

        oss << " ‚Üí RUN";
        addToLog(oss.str());

        // –ë–ª–æ–∫–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø –∫ dictionary –≤–æ –≤—Ä–µ–º—è compact
        std::lock_guard<std::mutex> lock(mapMutex);

        // === 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –Ω–æ–≤—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã ===
        std::vector<PostingList> newDict;
        newDict.reserve(dictionary.size() - holes);

        std::unordered_map<uint32_t, uint32_t> remap; // oldWid ‚Üí newWid

        // wordIds –ø–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –Ω–æ–≤—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        std::unordered_map<std::string, uint32_t> new_word2id;
        std::vector<std::string> new_id2word;

        size_t wordIdsSize = wordIds.size();
        for (uint32_t oldWid = 0; oldWid < dictionary.size(); ++oldWid)
        {
            auto& post = dictionary[oldWid];
            if (post.empty())
                continue; // –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—ã—Ä–∫—É

            // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ oldWid —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ wordIds
            if (oldWid >= wordIdsSize) {
                addToLog("COMPACT: WARNING - oldWid " + std::to_string(oldWid) + 
                        " >= wordIds.size() (" + std::to_string(wordIdsSize) + "), skipping");
                continue;
            }

            auto newWid = static_cast<uint32_t>(newDict.size());
            remap[oldWid] = newWid;
            newDict.push_back(std::move(post));

            // –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –ø–æ ID
            const std::string& word = wordIds.byId(oldWid);
            new_word2id[word] = newWid;
            new_id2word.push_back(word);
        }

        // –ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞—Ä—å
        dictionary.swap(newDict);

        // === 2. –ü–µ—Ä–µ—Å–±–æ—Ä–∫–∞ wordIds ===
        wordIds.rebuild(std::move(new_word2id), std::move(new_id2word));

        // === 3. –ü–µ—Ä–µ—Å–±–æ—Ä–∫–∞ wordRefs —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö ID ===
        size_t remapErrors = 0;
        for (auto& [fileId, vec] : wordRefs)
        {
            for (uint32_t& wid : vec)
            {
                auto it = remap.find(wid);
                if (it != remap.end()) {
                    wid = it->second;
                } else {
                    // –ï—Å–ª–∏ wid –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ remap, —ç—Ç–æ –æ—à–∏–±–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                    remapErrors++;
                    addToLog("COMPACT: ERROR - wid " + std::to_string(wid) + 
                            " not found in remap for fileId " + std::to_string(fileId));
                    // –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ—á–∏—Å—Ç–∫–∏
                    wid = UINT32_MAX;
                }
            }
        }

        // –û—á–∏—â–∞–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ wordRefs
        if (remapErrors > 0) {
            const uint32_t INVALID_WID = UINT32_MAX;
            for (auto& [fileId, vec] : wordRefs) {
                vec.erase(
                    std::remove(vec.begin(), vec.end(), INVALID_WID),
                    vec.end()
                );
            }
            addToLog("COMPACT: removed " + std::to_string(remapErrors) + 
                    " invalid word references");
        }

        // === 4. –ü–µ—Ä–µ—Å–±–æ—Ä–∫–∞ chunks –∏–∑ dictionary ===
        rebuildChunksFromDictionary();

        addToLog("COMPACT: done, new dict size=" + std::to_string(dictionary.size()) +
                ", new wordIds size=" + std::to_string(wordIds.size()));
    });
}

void inverted_index::InvertedIndex::safeEraseFile(FileId fileId)
{
    /*  –¥–ª—è –∫–∞–∂–¥–æ–π Wid-–æ—á–µ—Ä–µ–¥–∏ —Å—Ç–∏—Ä–∞–µ–º –∑–∞–ø–∏—Å—å hash, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
        (–Ω–∏–∫–∞–∫–∏—Ö erase –ø–æ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É –∏–Ω–¥–µ–∫—Å—É)  */

    boost::asio::post(strand_,[this,fileId]()
    {
        safeEraseFileInternal(fileId);
    });

}

bool inverted_index::InvertedIndex::enqueueFileUpdate(const std::wstring& path)
{
    namespace fs = std::filesystem;

    std::wcout << L"[enqueueFileUpdate] Request for path: " << path << std::endl;

    if (!fs::exists(path) || !fs::is_regular_file(path))
    {
        std::wcout << L"[enqueueFileUpdate] Skip ‚Äî not a regular file: " << path << std::endl;
        return false;
    }

    // –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∏—Ç–∞–µ–º –≤ –ª—é–±–æ–º –ø–æ—Ç–æ–∫–µ
    fs::file_time_type ts = fs::last_write_time(path);
    uint64_t sz = fs::file_size(path);

    //
    // –†–∞–±–æ—Ç–∞ —Å docPaths –∏ —É–¥–∞–ª–µ–Ω–∏–µ ‚Äî —Å—Ç—Ä–æ–≥–æ –≤ strand
    //
    boost::asio::post(strand_,
                      [this, path, ts, sz]()
                      {
                          std::wcout << L"[strand/docPaths] Upserting: " << path << std::endl;

                          auto [fileId, changed] = docPaths.upsert(path, ts, sz);

                          if (!changed)
                          {
                              std::wcout << L"[strand/docPaths] File unchanged ‚Äî stopping update.\n";
                              return;
                          }

                          bool isNewFile = (wordRefs.find(fileId) == wordRefs.end());

                          if (!isNewFile)
                          {
                              std::wcout << L"[strand/index] Old file changed, erasing old postings...\n";
                              safeEraseFileInternal(fileId);
                          }
                          else
                          {
                              std::wcout << L"[strand/index] New file detected, skipping erase.\n";
                          }

                          //
                          // ---- –°–æ–∑–¥–∞—ë–º –ø—Ä–æ–º–∏—Å –∏ future –î–õ–Ø –≠–¢–û–ì–û –ö–û–ù–ö–†–ï–¢–ù–û–ì–û –§–ê–ô–õ–ê ----
                          //
                          auto promise = std::make_shared<std::promise<void>>();
                          auto future  = promise->get_future();

                          // –ó–¥–µ—Å—å –ø–æ –∂–µ–ª–∞–Ω–∏—é –º–æ–∂–Ω–æ future —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ç–≤–æ—ë–º –º–µ–Ω–µ–¥–∂–µ—Ä–µ
                          // –∏–ª–∏ –≤–µ—Ä–Ω—É—Ç—å –Ω–∞—Ä—É–∂—É, –µ—Å–ª–∏ enqueueFileUpdate –±—É–¥–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å future.
                          // –ü–æ–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–∞–∫, future –±—É–¥–µ—Ç —É–Ω–∏—á—Ç–æ–∂–µ–Ω ‚Üí –Ω–æ—Ä–º–∞–ª—å–Ω–æ.

                          //
                          // ---- –ó–∞–ø—É—Å–∫–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Ñ–∞–π–ª–∞ ----
                          //
                          std::wcout << L"[strand/index] Scheduling fileIndexing job for id=" << fileId << std::endl;

                          boost::asio::post(cpu_pool_, [this, fileId, promise]()
                          {
                              try
                              {
                                  std::wcout << L"[fileIndexing] Started for id=" << fileId << std::endl;

                                  // üî• –í–ê–ñ–ù–û: fileIndexing —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç promise
                                  fileIndexing(fileId, promise);

                                  std::wcout << L"[fileIndexing] Dispatched batch for id=" << fileId << std::endl;
                              }
                              catch (...)
                              {
                                  std::wcerr << L"[fileIndexing EXC] id=" << fileId << std::endl;
                                  try { promise->set_exception(std::current_exception()); } catch(...) {}
                              }
                          });

                          //
                          // –¢–µ–ø–µ—Ä—å fileIndexing ‚Üí —Å–æ–∑–¥–∞—ë—Ç batch ‚Üí batch.promise = promise
                          // ‚Üí processBatch –∑–∞–≤–µ—Ä—à–∏—Ç promise->set_value() –∏–ª–∏ set_exception()
                          //
                      });

    return true;
}


bool inverted_index::InvertedIndex::enqueueFileDeletion(const std::wstring& path)
{
    std::wcout << L"[enqueueFileDeletion] Request for path: " << path << std::endl;

    FileId id;

    {
        std::lock_guard<std::mutex> lock(mapMutex); // —Ç–æ–ª—å–∫–æ –¥–ª—è tryGetId, –µ—Å–ª–∏ –æ–Ω–æ —Ç—Ä–µ–±—É–µ—Ç
        if (!docPaths.tryGetId(path, id))
        {
            std::wcout << L"[enqueueFileDeletion] Skip ‚Äî unknown file: " << path << std::endl;
            return false;
        }
    }

    // –ø–æ–º–µ—á–∞–µ–º —Ñ–∞–π–ª –∫–∞–∫ —É–¥–∞–ª—ë–Ω–Ω—ã–π
    boost::asio::post(strand_, [this, id, path]() {

        std::wcout << L"[strand/docPaths] markRemoved: " << path << L" (id=" << id << L")" << std::endl;
        docPaths.markRemoved(id);

    });

    // —á–∏—Å—Ç–∏–º –ø–æ—Å—Ç–∏–Ω–≥–∏ –∏ wordRefs
    boost::asio::post(strand_, [this, id]() {

        std::wcout << L"[strand/dictionary] Starting cleanup for id=" << id << std::endl;

        auto refIt = wordRefs.find(id);
        if (refIt == wordRefs.end()) {
            std::wcout << L"[strand/dictionary] Already removed, id=" << id << std::endl;
            return;
        }

        const auto &widList = refIt->second;

        for (uint32_t wid : widList)
        {
            const size_t chunkIndex = wid / CHUNK_SIZE;
            const size_t localIndex = wid % CHUNK_SIZE;

            if (chunkIndex >= dictionaryChunks.size())
                continue;

            auto& chunkPtr = dictionaryChunks[chunkIndex];
            if (!chunkPtr)
                continue;

            Chunk& chunk = *chunkPtr;
            std::unique_lock<std::shared_mutex> lk(chunk.mutex);

            auto& posting = chunk.bucket[localIndex];
            posting.erase(id);
            // –µ—Å–ª–∏ –ø—É—Å—Ç–æ–π ‚Äî –ø—Ä–æ—Å—Ç–æ –æ—Å—Ç–∞—ë—Ç—Å—è –ø—É—Å—Ç—ã–º; —á–∏—Å—Ç–∏—Ç—å –µ—â—ë –≥–¥–µ-—Ç–æ –Ω–µ –Ω–∞–¥–æ
        }

        std::wcout << L"[strand/dictionary] Cleaned "
                   << widList.size() << L" postings for id=" << id << std::endl;

        auto it = wordRefs.find(id);
        if (it != wordRefs.end()) {
            wordRefs.erase(it);
            std::wcout << L"[strand/wordRefs] Erased id=" << id << std::endl;
        } else {
            std::wcout << L"[strand/wordRefs] Already erased id=" << id << std::endl;
        }
    });

    return true;
}
